{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport torch.nn.functional as F\nimport torch.optim.lr_scheduler as lr_scheduler\n\n# 定义卷积神经网络模型\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n\n        # 第一个卷积层：输入通道数为1（因为MNIST是灰度图像），输出通道数为32，\n        # 卷积核大小为3x3，步长为1，填充为1\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n\n        # 第二个卷积层：输入通道数为32，输出通道数为64，\n        # 卷积核大小为3x3，步长为1，填充为1\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n\n        # 最大池化层：池化核大小为2x2，步长为2，无填充\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n\n        # 全连接层1：输入特征数为64 * 7 * 7（通过两次池化后的特征图大小为7x7）\n        # 输出特征数为128\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n\n        self.fc2 = nn.Linear(128, 10)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # 前向传播函数，定义数据在网络中的流动路径\n\n        x = self.relu(self.conv1(x))\n\n        x = self.pool(x)\n\n        x = self.relu(self.conv2(x))\n\n        x = self.pool(x)\n\n        x = x.view(-1, 64 * 7 * 7)\n        #逆向\n        #tensor.view(batch_size,64,7,7)\n\n        x = self.relu(self.fc1(x))\n\n        x = self.fc2(x)\n\n        return x  # 使用LogSoftmax作为最后一层的激活函数\n\n\n# 数据预处理和加载\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n\n# 检查 GPU 是否可用\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    #model=Net_digits().cuda()#同model.to(device)\n    print(\"CUDA is available. Using GPU.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"CUDA is not available. Using CPU.\")\n\n\n# 初始化模型、损失函数和优化器\nmodel = CNNModel().to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=4, factor=0.5,min_lr=0,verbose=True)\n\n# 训练模型\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    for data, target in train_loader:\n        data,target=data.to(device),target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n    # 在测试集上评估模型性能\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data,target=data.to(device),target.to(device)\n            output = model(data)\n            loss_test=criterion(output,target.squeeze().long())\n            _, predicted = torch.max(output.data, 1)\n            predicted=predicted\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n    accuracy = correct / total\n    scheduler.step(accuracy)\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss_test}, Test Accuracy: {accuracy * 100:.2f}%')\n    if accuracy>=0.997: #早停机制\n        print(f'Early stopping. Test Accuracy reached {accuracy * 100:.2f}%')\n        break\n\n# 输出最终模型在测试集上的准确率\nprint(f'Final Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T06:40:06.604507Z","iopub.execute_input":"2024-08-01T06:40:06.604839Z","iopub.status.idle":"2024-08-01T06:40:54.112670Z","shell.execute_reply.started":"2024-08-01T06:40:06.604812Z","shell.execute_reply":"2024-08-01T06:40:54.111731Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"CUDA is available. Using GPU.\nEpoch 1/3, Loss: 0.0022243682760745287, Test Accuracy: 98.54%\nEpoch 2/3, Loss: 0.000849008618388325, Test Accuracy: 98.53%\nEpoch 3/3, Loss: 3.4717733797151595e-05, Test Accuracy: 97.95%\nFinal Test Accuracy: 97.95%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 适配任务","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#展示所有行和列\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns',None)\npredict=pd.read_csv(r'/kaggle/input/digit-recognizer/test.csv',encoding='utf8')\ntrain=pd.read_csv(r'/kaggle/input/digit-recognizer/train.csv',encoding='utf8')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:45:27.418491Z","iopub.execute_input":"2024-08-01T07:45:27.419363Z","iopub.status.idle":"2024-08-01T07:45:31.662502Z","shell.execute_reply.started":"2024-08-01T07:45:27.419325Z","shell.execute_reply":"2024-08-01T07:45:31.661440Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,test=train_test_split(train,test_size=0.2,random_state=42)\nprint(train.shape,test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:45:31.664523Z","iopub.execute_input":"2024-08-01T07:45:31.664809Z","iopub.status.idle":"2024-08-01T07:45:31.879923Z","shell.execute_reply.started":"2024-08-01T07:45:31.664783Z","shell.execute_reply":"2024-08-01T07:45:31.879005Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"(33600, 785) (8400, 785)\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train=train.drop(['label'],axis=1)\ny_train=train['label']\nx_test=test.drop(['label'],axis=1)\ny_test=test['label']","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:45:31.881119Z","iopub.execute_input":"2024-08-01T07:45:31.881380Z","iopub.status.idle":"2024-08-01T07:45:31.965878Z","shell.execute_reply.started":"2024-08-01T07:45:31.881356Z","shell.execute_reply":"2024-08-01T07:45:31.964811Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\n\nxtrain_tensor=torch.tensor(x_train.to_numpy(),dtype=torch.float32)\nytrain_tensor=torch.tensor(y_train.to_numpy(),dtype=torch.float32)\nytrain_tensor=ytrain_tensor.unsqueeze(1)\n\nxtest_tensor=torch.tensor(x_test.to_numpy(),dtype=torch.float32)\nytest_tensor=torch.tensor(y_test.to_numpy(),dtype=torch.float32)\nytest_tensor=ytest_tensor.unsqueeze(1)\n\nclass CustomDataset(Dataset):\n    def __init__(self, features, labels):\n        self.features = features\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, index):\n        x = self.features[index]\n        y = self.labels[index]\n        return x, y\n    \nclass Predict_set(Dataset):\n    def __init__(self, features):\n        self.features = torch.tensor(np.array(features),dtype=torch.float32)\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, index):\n        x = self.features[index]\n        return x\n    \npredict_set = Predict_set(predict)\npredict_loader = DataLoader(predict_set,batch_size=64)\n    \npredict_set = Predict_set(predict)\npredict_loader = DataLoader(predict_set,batch_size=64)\n\ntrain_set=CustomDataset(xtrain_tensor,ytrain_tensor)\ntest_set=CustomDataset(xtest_tensor,ytest_tensor)\n\ntrain_loader=DataLoader(train_set,batch_size=64,shuffle=True)\ntest_loader=DataLoader(test_set,batch_size=64,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:45:31.967141Z","iopub.execute_input":"2024-08-01T07:45:31.967907Z","iopub.status.idle":"2024-08-01T07:45:32.236037Z","shell.execute_reply.started":"2024-08-01T07:45:31.967870Z","shell.execute_reply":"2024-08-01T07:45:32.234988Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim.lr_scheduler as lr_scheduler\n\n\nclass DigitNet(nn.Module):\n    \n    def __init__(self):\n        super(DigitNet,self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n        \n        self.fc1=nn.Linear(64*7*7,128)\n        \n        self.fc2=nn.Linear(128,10)\n        \n        self.relu=nn.ReLU()\n        \n        self.norm = nn.BatchNorm1d(64 * 7 * 7) # 对数据进行归一化\n        \n    def forward(self,x):\n        x = x.view(-1,1,28,28)\n#         x = x.reshape(-1,1,28,28)\n        \n        x = self.relu(self.conv1(x))\n        \n        x = self.pool(x)\n        \n        x = self.relu(self.conv2(x))\n        \n        x = self.pool(x)\n        \n        x = x.view(-1, 64 * 7 * 7)\n        \n        x = self.norm(x)\n        \n        #逆向\n        #tensor.view(batch_size,64,7,7)\n        \n        x = self.relu(self.fc1(x))\n        \n        x = self.fc2(x)\n        \n        return x\n    \nif torch.cuda.is_available():\n    device=torch.device(\"cuda\")\n    print('CUDA is available,using GPU')\nelse:\n    device=torch.device(\"cpu\")\n    print('CUDA is notfind,using CPU')\n    \nmodel=DigitNet().to(device)\n# criterion=nn.BCELoss().to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer=torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=4, factor=0.5,min_lr=0,verbose=True)\n\n\nepochs=30\n\nfor epoch in range(epochs):\n    model.train\n    for data,target in train_loader:\n        data,target=data.to(device),target.to(device)\n        optimizer.zero_grad()\n        output=model(data)\n#         target=target.squeeze(1)\n        loss=criterion(output,target.squeeze(1).long())\n        loss.backward()\n        optimizer.step()\n        \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        # 设置模型为评估模式\n        model.eval()\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss_test=criterion(output,target.squeeze(1).long())\n            _, predicted = torch.max(output, dim=1)\n            total += target.size(0)\n            correct += (predicted == target.squeeze()).sum().item()\n        accuracy = correct / total\n        scheduler.step(accuracy) # 学习率调度器\n\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss_test}, Test Accuracy: {accuracy * 100:.2f}%')\n    if accuracy>=0.997: #早停机制\n        print(f'Early stopping. Test Accuracy reached {accuracy * 100:.2f}%')\n        break\n\n# 输出最终模型在测试集上的准确率\nprint(f'Final Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:53:37.386969Z","iopub.execute_input":"2024-08-01T07:53:37.387690Z","iopub.status.idle":"2024-08-01T07:54:36.993547Z","shell.execute_reply.started":"2024-08-01T07:53:37.387658Z","shell.execute_reply":"2024-08-01T07:54:36.992445Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"CUDA is available,using GPU\nEpoch 1/30, Loss: 0.017569325864315033, Test Accuracy: 97.25%\nEpoch 2/30, Loss: 0.011772358790040016, Test Accuracy: 98.39%\nEpoch 3/30, Loss: 0.001014163251966238, Test Accuracy: 98.37%\nEpoch 4/30, Loss: 0.01566229574382305, Test Accuracy: 98.49%\nEpoch 5/30, Loss: 0.024510174989700317, Test Accuracy: 98.42%\nEpoch 6/30, Loss: 0.022524051368236542, Test Accuracy: 98.62%\nEpoch 7/30, Loss: 0.005927287507802248, Test Accuracy: 98.25%\nEpoch 8/30, Loss: 5.051389962318353e-06, Test Accuracy: 98.76%\nEpoch 9/30, Loss: 0.03502592071890831, Test Accuracy: 98.60%\nEpoch 10/30, Loss: 0.0003023294557351619, Test Accuracy: 98.54%\nEpoch 11/30, Loss: 9.996775770559907e-05, Test Accuracy: 98.71%\nEpoch 12/30, Loss: 0.0006100759492255747, Test Accuracy: 98.45%\nEpoch 13/30, Loss: 5.990127192490036e-06, Test Accuracy: 98.82%\nEpoch 14/30, Loss: 0.00017315434524789453, Test Accuracy: 98.52%\nEpoch 15/30, Loss: 7.4204590418958105e-06, Test Accuracy: 98.87%\nEpoch 16/30, Loss: 2.7790299554908415e-06, Test Accuracy: 98.89%\nEpoch 17/30, Loss: 0.006916450802236795, Test Accuracy: 98.65%\nEpoch 18/30, Loss: 1.1175833378729294e-06, Test Accuracy: 98.83%\nEpoch 19/30, Loss: 5.215399028202228e-07, Test Accuracy: 98.79%\nEpoch 20/30, Loss: 6.5040221670642495e-06, Test Accuracy: 98.82%\nEpoch 00021: reducing learning rate of group 0 to 5.0000e-04.\nEpoch 21/30, Loss: 0.0009732343605719507, Test Accuracy: 98.69%\nEpoch 22/30, Loss: 0.0007022360223345459, Test Accuracy: 98.88%\nEpoch 23/30, Loss: 3.8879945350345224e-05, Test Accuracy: 98.99%\nEpoch 24/30, Loss: 2.4105433112708852e-05, Test Accuracy: 98.96%\nEpoch 25/30, Loss: 2.0068644516868517e-05, Test Accuracy: 98.98%\nEpoch 26/30, Loss: 1.6769008652772754e-05, Test Accuracy: 99.01%\nEpoch 27/30, Loss: 1.4228990949050058e-05, Test Accuracy: 99.02%\nEpoch 28/30, Loss: 1.1837851161544677e-05, Test Accuracy: 99.05%\nEpoch 29/30, Loss: 9.372127351525705e-06, Test Accuracy: 99.06%\nEpoch 30/30, Loss: 7.591680514451582e-06, Test Accuracy: 99.06%\nFinal Test Accuracy: 99.06%\n","output_type":"stream"}]},{"cell_type":"code","source":"result_df = pd.DataFrame()\nmodel.eval()\nfor i in predict_loader:\n    input_p = i.to(device)\n    output_p = model(input_p)\n    _, output_p = torch.max(output_p, dim=1)\n    output_p_df = pd.DataFrame(np.array(output_p.detach().cpu()))\n    result_df = pd.concat([result_df,output_p_df],axis=0)\nresult_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:54:58.274353Z","iopub.execute_input":"2024-08-01T07:54:58.274732Z","iopub.status.idle":"2024-08-01T07:54:58.929770Z","shell.execute_reply.started":"2024-08-01T07:54:58.274698Z","shell.execute_reply":"2024-08-01T07:54:58.928853Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"   0\n0  2\n1  0\n2  9\n3  9\n4  3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"result_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:08:15.611767Z","iopub.execute_input":"2024-08-01T07:08:15.612139Z","iopub.status.idle":"2024-08-01T07:08:15.618126Z","shell.execute_reply.started":"2024-08-01T07:08:15.612105Z","shell.execute_reply":"2024-08-01T07:08:15.617232Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"(28000, 10)"},"metadata":{}}]},{"cell_type":"markdown","source":"# 直接从for_ward中返回网络输出结果和索引","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.nn.functional as F\n\n\nclass DigitNet(nn.Module):\n    \n    def __init__(self):\n        super(DigitNet,self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n        \n        self.fc1=nn.Linear(64*7*7,128)\n        \n        self.fc2=nn.Linear(128,10)\n        \n        self.relu=nn.ReLU()\n        \n        self.norm = nn.BatchNorm1d(64 * 7 * 7) # 对数据进行归一化\n        \n    def forward(self,x):\n        x = x.view(-1,1,28,28)\n#         x = x.reshape(-1,1,28,28)\n        \n        x = self.relu(self.conv1(x))\n        \n        x = self.pool(x)\n        \n        x = self.relu(self.conv2(x))\n        \n        x = self.pool(x)\n        \n        x = x.view(-1, 64 * 7 * 7)\n        \n        x = self.norm(x)\n        \n        #逆向\n        #tensor.view(batch_size,64,7,7)\n        \n        x = self.relu(self.fc1(x))\n        \n        x = self.fc2(x)\n        \n#         output = F.log_softmax(x, dim=1)\n        \n        _, predicted = torch.max(x, dim=1)\n        \n        return x, predicted\n    \nif torch.cuda.is_available():\n    device=torch.device(\"cuda\")\n    print('CUDA is available,using GPU')\nelse:\n    device=torch.device(\"cpu\")\n    print('CUDA is notfind,using CPU')\n    \nmodel=DigitNet().to(device)\n# criterion=nn.BCELoss().to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer=torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=4, factor=0.5,min_lr=0,verbose=True)\n\n\nepochs=10\n\nfor epoch in range(epochs):\n    model.train\n    for data,target in train_loader:\n        data,target = data.to(device),target.to(device)\n        optimizer.zero_grad()\n        output,_ = model(data)\n        loss=criterion(output,target.squeeze(1).long())\n        loss.backward()\n        optimizer.step()\n        \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        # 设置模型为评估模式\n        model.eval()\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output,predicted = model(data)\n            loss_test=criterion(output,target.squeeze(1).long())\n            total += target.size(0)\n            correct += (predicted == target.squeeze()).sum().item()\n        accuracy = correct / total\n        scheduler.step(accuracy) # 学习率调度器\n\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss_test}, Test Accuracy: {accuracy * 100:.2f}%')\n    if accuracy>=0.997: #早停机制\n        print(f'Early stopping. Test Accuracy reached {accuracy * 100:.2f}%')\n        break\n\n# 输出最终模型在测试集上的准确率\nprint(f'Final Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:57:38.939325Z","iopub.execute_input":"2024-08-01T07:57:38.940059Z","iopub.status.idle":"2024-08-01T07:57:59.040063Z","shell.execute_reply.started":"2024-08-01T07:57:38.940027Z","shell.execute_reply":"2024-08-01T07:57:59.039111Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"CUDA is available,using GPU\nEpoch 1/10, Loss: 0.006916174665093422, Test Accuracy: 97.42%\nEpoch 2/10, Loss: 0.06900906562805176, Test Accuracy: 97.85%\nEpoch 3/10, Loss: 0.20799463987350464, Test Accuracy: 98.26%\nEpoch 4/10, Loss: 0.0036278641782701015, Test Accuracy: 98.75%\nEpoch 5/10, Loss: 0.04211093857884407, Test Accuracy: 98.54%\nEpoch 6/10, Loss: 0.0015080865705385804, Test Accuracy: 98.76%\nEpoch 7/10, Loss: 0.0036886176094412804, Test Accuracy: 98.39%\nEpoch 8/10, Loss: 0.00016015541041269898, Test Accuracy: 98.73%\nEpoch 9/10, Loss: 0.04492902755737305, Test Accuracy: 98.54%\nEpoch 10/10, Loss: 0.002489992883056402, Test Accuracy: 98.76%\nFinal Test Accuracy: 98.76%\n","output_type":"stream"}]},{"cell_type":"code","source":"result_df = pd.DataFrame()\nmodel.eval()\nfor i in predict_loader:\n    input_p = i.to(device)\n    _,output_p = model(input_p)\n    output_p_df = pd.DataFrame(np.array(output_p.detach().cpu()))\n    result_df = pd.concat([result_df,output_p_df],axis=0)\nresult_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:58:08.604850Z","iopub.execute_input":"2024-08-01T07:58:08.605641Z","iopub.status.idle":"2024-08-01T07:58:09.277884Z","shell.execute_reply.started":"2024-08-01T07:58:08.605606Z","shell.execute_reply":"2024-08-01T07:58:09.276893Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"   0\n0  2\n1  0\n2  9\n3  0\n4  3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"result_df.to_csv(\"Digit_Recognizer.csv\",sep=',',encoding='utf8')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T08:01:42.327181Z","iopub.execute_input":"2024-08-01T08:01:42.327882Z","iopub.status.idle":"2024-08-01T08:01:42.391755Z","shell.execute_reply.started":"2024-08-01T08:01:42.327849Z","shell.execute_reply":"2024-08-01T08:01:42.390831Z"},"trusted":true},"execution_count":66,"outputs":[]}]}